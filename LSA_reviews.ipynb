{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dpal2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dpal2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import nltk \n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Coding\\Major Project\\Fake-Product-Review-Monitoring-master\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Coding\\Major Project\\Fake-Product-Review-Monitoring-master\\.venv\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "c:\\Coding\\Major Project\\Fake-Product-Review-Monitoring-master\\.venv\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "c:\\Coding\\Major Project\\Fake-Product-Review-Monitoring-master\\.venv\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained models and data\n",
    "import pickle\n",
    "classifier = pickle.load(open('classifier.pickle', 'rb'))\n",
    "vectorizer = pickle.load(open('TfidfModel.pickle', 'rb'))\n",
    "# X and y are training data, may not be needed for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Coding\\Major Project\\Fake-Product-Review-Monitoring-master\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3553: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>IP Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>20422322</td>\n",
       "      <td>R8MEA6IGAHO0B</td>\n",
       "      <td>B00MC4CED8</td>\n",
       "      <td>82850235</td>\n",
       "      <td>BlackVue DR600GW-PMP</td>\n",
       "      <td>Mobile_Electronics</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Very Happy!</td>\n",
       "      <td>As advertised. Everything works perfectly, I'm...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1.440988e+09</td>\n",
       "      <td>193.93.167.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>40835037</td>\n",
       "      <td>R31LOQ8JGLPRLK</td>\n",
       "      <td>B00OQMFG1Q</td>\n",
       "      <td>82850235</td>\n",
       "      <td>GENSSI GSM / GPS Two Way Smart Phone Car Alarm...</td>\n",
       "      <td>Mobile_Electronics</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>five star</td>\n",
       "      <td>it's great</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1.441002e+09</td>\n",
       "      <td>193.93.167.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>51469641</td>\n",
       "      <td>R2Y0MM9YE6OP3P</td>\n",
       "      <td>B00QERR5CY</td>\n",
       "      <td>82850235</td>\n",
       "      <td>iXCC Multi pack Lightning cable</td>\n",
       "      <td>Mobile_Electronics</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>great cables</td>\n",
       "      <td>These work great and fit my life proof case fo...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1.440959e+09</td>\n",
       "      <td>193.93.167.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>4332923</td>\n",
       "      <td>RRB9C05HDOD4O</td>\n",
       "      <td>B00QUFTPV4</td>\n",
       "      <td>82850235</td>\n",
       "      <td>abcGoodefg® FBI Covert Acoustic Tube Earpiece ...</td>\n",
       "      <td>Mobile_Electronics</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Work very well</td>\n",
       "      <td>Work very well</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1.441015e+09</td>\n",
       "      <td>193.93.167.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>44855305</td>\n",
       "      <td>R26I2RI1GFV8QG</td>\n",
       "      <td>B0067XVNTG</td>\n",
       "      <td>563475445</td>\n",
       "      <td>Generic Car Dashboard Video Camera Vehicle Vid...</td>\n",
       "      <td>Mobile_Electronics</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Cameras has battery issues</td>\n",
       "      <td>Be careful with these products, I have bought ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1.440973e+09</td>\n",
       "      <td>205.10.168.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88573</th>\n",
       "      <td>US</td>\n",
       "      <td>38978459</td>\n",
       "      <td>R2CDRVDUKB5Z9P</td>\n",
       "      <td>B00005OTZQ</td>\n",
       "      <td>554527960</td>\n",
       "      <td>Royal SE 2800 Hand-Held Spot Cleaner</td>\n",
       "      <td>Mobile_Electronics</td>\n",
       "      <td>5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>The Cat Barf is Gone!</td>\n",
       "      <td>I've been looking for a while for the \\\\\"purr\\...</td>\n",
       "      <td>2002-04-03</td>\n",
       "      <td>1.017782e+09</td>\n",
       "      <td>201.30.21.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88574</th>\n",
       "      <td>US</td>\n",
       "      <td>51697602</td>\n",
       "      <td>R1DVLTZFXXOX9</td>\n",
       "      <td>B00005OTZQ</td>\n",
       "      <td>554527960</td>\n",
       "      <td>Royal SE 2800 Hand-Held Spot Cleaner</td>\n",
       "      <td>Mobile_Electronics</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Well worth [it]</td>\n",
       "      <td>We live in an apartment with hardwood floors a...</td>\n",
       "      <td>2002-03-05</td>\n",
       "      <td>1.015341e+09</td>\n",
       "      <td>216.59.233.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88575</th>\n",
       "      <td>US</td>\n",
       "      <td>50891675</td>\n",
       "      <td>R1GHKKZMHAHQC1</td>\n",
       "      <td>B00005OTZQ</td>\n",
       "      <td>554527960</td>\n",
       "      <td>Royal SE 2800 Hand-Held Spot Cleaner</td>\n",
       "      <td>Mobile_Electronics</td>\n",
       "      <td>5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Its a Time saver!!!</td>\n",
       "      <td>I received this for a birthday present and Its...</td>\n",
       "      <td>2002-02-19</td>\n",
       "      <td>1.014068e+09</td>\n",
       "      <td>209.193.182.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88576</th>\n",
       "      <td>US</td>\n",
       "      <td>38528439</td>\n",
       "      <td>R227G6T5B26DVC</td>\n",
       "      <td>B00005OTZQ</td>\n",
       "      <td>554527960</td>\n",
       "      <td>Royal SE 2800 Hand-Held Spot Cleaner</td>\n",
       "      <td>Mobile_Electronics</td>\n",
       "      <td>5</td>\n",
       "      <td>58.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Got Stains?</td>\n",
       "      <td>I got the Dirt Devil Spot Scrubber for a gift ...</td>\n",
       "      <td>2002-01-06</td>\n",
       "      <td>1.010282e+09</td>\n",
       "      <td>205.32.108.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88577</th>\n",
       "      <td>US</td>\n",
       "      <td>47099634</td>\n",
       "      <td>R44WS2RSQQ0HR</td>\n",
       "      <td>B00005OTZQ</td>\n",
       "      <td>554527960</td>\n",
       "      <td>Royal SE 2800 Hand-Held Spot Cleaner</td>\n",
       "      <td>Mobile_Electronics</td>\n",
       "      <td>4</td>\n",
       "      <td>37.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>no smell has to mean it is getting the job don...</td>\n",
       "      <td>I purchased this about three weeks ago along w...</td>\n",
       "      <td>2001-12-22</td>\n",
       "      <td>1.009026e+09</td>\n",
       "      <td>222.92.68.154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88578 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0              US     20422322   R8MEA6IGAHO0B  B00MC4CED8        82850235   \n",
       "1              US     40835037  R31LOQ8JGLPRLK  B00OQMFG1Q        82850235   \n",
       "2              US     51469641  R2Y0MM9YE6OP3P  B00QERR5CY        82850235   \n",
       "3              US      4332923   RRB9C05HDOD4O  B00QUFTPV4        82850235   \n",
       "4              US     44855305  R26I2RI1GFV8QG  B0067XVNTG       563475445   \n",
       "...           ...          ...             ...         ...             ...   \n",
       "88573          US     38978459  R2CDRVDUKB5Z9P  B00005OTZQ       554527960   \n",
       "88574          US     51697602   R1DVLTZFXXOX9  B00005OTZQ       554527960   \n",
       "88575          US     50891675  R1GHKKZMHAHQC1  B00005OTZQ       554527960   \n",
       "88576          US     38528439  R227G6T5B26DVC  B00005OTZQ       554527960   \n",
       "88577          US     47099634   R44WS2RSQQ0HR  B00005OTZQ       554527960   \n",
       "\n",
       "                                           product_title    product_category  \\\n",
       "0                                   BlackVue DR600GW-PMP  Mobile_Electronics   \n",
       "1      GENSSI GSM / GPS Two Way Smart Phone Car Alarm...  Mobile_Electronics   \n",
       "2                        iXCC Multi pack Lightning cable  Mobile_Electronics   \n",
       "3      abcGoodefg® FBI Covert Acoustic Tube Earpiece ...  Mobile_Electronics   \n",
       "4      Generic Car Dashboard Video Camera Vehicle Vid...  Mobile_Electronics   \n",
       "...                                                  ...                 ...   \n",
       "88573               Royal SE 2800 Hand-Held Spot Cleaner  Mobile_Electronics   \n",
       "88574               Royal SE 2800 Hand-Held Spot Cleaner  Mobile_Electronics   \n",
       "88575               Royal SE 2800 Hand-Held Spot Cleaner  Mobile_Electronics   \n",
       "88576               Royal SE 2800 Hand-Held Spot Cleaner  Mobile_Electronics   \n",
       "88577               Royal SE 2800 Hand-Held Spot Cleaner  Mobile_Electronics   \n",
       "\n",
       "      star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0               5            0.0          0.0    N                 Y   \n",
       "1               5            0.0          1.0    N                 Y   \n",
       "2               5            0.0          0.0    N                 Y   \n",
       "3               4            0.0          0.0    N                 Y   \n",
       "4               2            0.0          0.0    N                 Y   \n",
       "...           ...            ...          ...  ...               ...   \n",
       "88573           5           23.0         27.0    N                 N   \n",
       "88574           5           10.0         11.0    N                 N   \n",
       "88575           5           13.0         13.0    N                 N   \n",
       "88576           5           58.0         61.0    N                 N   \n",
       "88577           4           37.0         38.0    N                 N   \n",
       "\n",
       "                                         review_headline  \\\n",
       "0                                            Very Happy!   \n",
       "1                                              five star   \n",
       "2                                           great cables   \n",
       "3                                         Work very well   \n",
       "4                             Cameras has battery issues   \n",
       "...                                                  ...   \n",
       "88573                              The Cat Barf is Gone!   \n",
       "88574                                    Well worth [it]   \n",
       "88575                                Its a Time saver!!!   \n",
       "88576                                        Got Stains?   \n",
       "88577  no smell has to mean it is getting the job don...   \n",
       "\n",
       "                                             review_body review_date  \\\n",
       "0      As advertised. Everything works perfectly, I'm...  2015-08-31   \n",
       "1                                             it's great  2015-08-31   \n",
       "2      These work great and fit my life proof case fo...  2015-08-31   \n",
       "3                                        Work very well   2015-08-31   \n",
       "4      Be careful with these products, I have bought ...  2015-08-31   \n",
       "...                                                  ...         ...   \n",
       "88573  I've been looking for a while for the \\\\\"purr\\...  2002-04-03   \n",
       "88574  We live in an apartment with hardwood floors a...  2002-03-05   \n",
       "88575  I received this for a birthday present and Its...  2002-02-19   \n",
       "88576  I got the Dirt Devil Spot Scrubber for a gift ...  2002-01-06   \n",
       "88577  I purchased this about three weeks ago along w...  2001-12-22   \n",
       "\n",
       "          timestamp      IP Address  \n",
       "0      1.440988e+09   193.93.167.87  \n",
       "1      1.441002e+09   193.93.167.87  \n",
       "2      1.440959e+09   193.93.167.87  \n",
       "3      1.441015e+09   193.93.167.87  \n",
       "4      1.440973e+09   205.10.168.66  \n",
       "...             ...             ...  \n",
       "88573  1.017782e+09   201.30.21.211  \n",
       "88574  1.015341e+09   216.59.233.79  \n",
       "88575  1.014068e+09  209.193.182.91  \n",
       "88576  1.010282e+09  205.32.108.104  \n",
       "88577  1.009026e+09   222.92.68.154  \n",
       "\n",
       "[88578 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"reviews.csv\",sep=\"\\t\")\n",
    "dataset.drop(columns=\"Unnamed: 0\", inplace=True, errors=\"ignore\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "import collections\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_index(\"review_id\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent symantic analysis\n",
    "# it will analyse all reviews and determine all reviews belong to the same concept\n",
    "def LSA(text):\n",
    "    #text is list of reviews of same product\n",
    "    \n",
    "    \n",
    "    # Created TF-IDF Model\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(text)\n",
    "\n",
    "    # Skip LSA if there is no meaningful variance\n",
    "    if X.shape[0] < 2 or X.shape[1] < 2 or X.nnz == 0:\n",
    "        return [0.0] * len(text)\n",
    "    \n",
    "    # Created SVD(Singular Value Decomposition)\n",
    "    # suppress RuntimeWarning caused by zero variance matrices\n",
    "    import warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "        lsa = TruncatedSVD(n_components = 1, n_iter = 100, random_state=42)\n",
    "        lsa.fit(X)\n",
    "    \n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    concept_words={}\n",
    "\n",
    "    for j,comp in enumerate(lsa.components_):\n",
    "        componentTerms = zip(terms,comp)\n",
    "        sortedTerms = sorted(componentTerms,key=lambda x:x[1],reverse=True)\n",
    "        sortedTerms = sortedTerms[:10]\n",
    "        concept_words[str(j)] = sortedTerms\n",
    "     \n",
    "    sentence_scores = []\n",
    "    for key in concept_words.keys():\n",
    "        for sentence in text:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            scores = 0\n",
    "            for word in words:\n",
    "                for word_with_scores in concept_words[key]:\n",
    "                    if word == word_with_scores[0]:\n",
    "                        scores += word_with_scores[1]\n",
    "            sentence_scores.append(scores)\n",
    "    return sentence_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\dpal2/nltk_data'\n    - 'c:\\\\Coding\\\\Major Project\\\\Fake-Product-Review-Monitoring-master\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Coding\\\\Major Project\\\\Fake-Product-Review-Monitoring-master\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Coding\\\\Major Project\\\\Fake-Product-Review-Monitoring-master\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\dpal2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16712\\2418168353.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\s+\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;31m#text into word list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Coding\\Major Project\\Fake-Product-Review-Monitoring-master\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Coding\\Major Project\\Fake-Product-Review-Monitoring-master\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Coding\\Major Project\\Fake-Product-Review-Monitoring-master\\.venv\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Coding\\Major Project\\Fake-Product-Review-Monitoring-master\\.venv\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Coding\\Major Project\\Fake-Product-Review-Monitoring-master\\.venv\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\dpal2/nltk_data'\n    - 'c:\\\\Coding\\\\Major Project\\\\Fake-Product-Review-Monitoring-master\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Coding\\\\Major Project\\\\Fake-Product-Review-Monitoring-master\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Coding\\\\Major Project\\\\Fake-Product-Review-Monitoring-master\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\dpal2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "product_df = dataset.groupby(\"product_id\")\n",
    "#grouping dataset by product_id\n",
    "\n",
    "unique_products = dataset[\"product_id\"].unique()\n",
    "#stores list of all product id\n",
    "\n",
    "no_products = len(unique_products.tolist())\n",
    "#no. of products\n",
    "\n",
    "remove_reviews = []\n",
    "#store review_id that are fake \n",
    "    \n",
    "for i in range(no_products):\n",
    "    #iterate through all product reviews \n",
    "    \n",
    "    df = product_df.get_group(unique_products[i])\n",
    "    #dataframe of a single product\n",
    "    \n",
    "    unique_reviews = df.index.tolist()\n",
    "    #list of review_id of reviews of same product\n",
    "    \n",
    "    no_reviews = len(unique_reviews)   \n",
    "    #no. of reviews of same product\n",
    "    \n",
    "    count = no_reviews \n",
    "    #count is no. of reviews that can be analysed\n",
    "    \n",
    "    reviews = []\n",
    "    #list of review texts\n",
    "    \n",
    "    review_id = []\n",
    "    #list of review texts\n",
    "    \n",
    "    for j in range(no_reviews):\n",
    "        #iterate through all reviews\n",
    "        \n",
    "        text = str(df.loc[unique_reviews[j]][\"review_body\"])\n",
    "        # text of a review \n",
    "        \n",
    "        #cleaning the text\n",
    "        text = re.sub(r\"\\W\",\" \",text)             \n",
    "        text = re.sub(r\"\\d\",\" \",text)             \n",
    "        text = re.sub(r\"\\s+[a-z]\\s+\",\" \",text)    \n",
    "        text = re.sub(r\"^[a-z]\\s+\",\" \",text)    \n",
    "        text = re.sub(r\"\\s+[a-z]$\",\" \",text)    \n",
    "        text = re.sub(r\"\\s+\",\" \",text)\n",
    "        \n",
    "        words = nltk.word_tokenize(text)\n",
    "        #text into word list\n",
    "        \n",
    "        if(len(words)==1):\n",
    "        #if only one word in text review\n",
    "            \n",
    "            if(len(text) <=1 or not wordnet.synsets(text) ):\n",
    "            #if word is having only one character or invalid english word\n",
    "                \n",
    "                remove_reviews.append(unique_reviews[j])\n",
    "                #append this review as fake\n",
    "                \n",
    "                count-=1\n",
    "                #review left to be analysed will be decrease by 1\n",
    "                \n",
    "                continue\n",
    "                #check for next review\n",
    "                '''\n",
    "            elif(len(words[0])<=1):\n",
    "                remove_reviews.append(unique_reviews[j])\n",
    "                count-=1\n",
    "                continue\n",
    "                '''\n",
    "        elif(len(words)==0):\n",
    "        #if no words\n",
    "            \n",
    "            remove_reviews.append(unique_reviews[j])\n",
    "            #append this review as fake\n",
    "            \n",
    "            count-=1\n",
    "            #review left to be analysed will be decrease by 1\n",
    "            \n",
    "            continue\n",
    "            #check for next review\n",
    "        \n",
    "        review_id.append(unique_reviews[j])\n",
    "        #valid: append review_id to review_id list for analysis\n",
    "        \n",
    "        reviews.append(text)\n",
    "        #valid: append review_body to reviews list for analysis\n",
    "        \n",
    "    ###########################################################################################\n",
    "    if(count<=0):\n",
    "        #if no reviews left to analyse \n",
    "\n",
    "        continue\n",
    "        #check for next\n",
    "        \n",
    "    if(count==1): \n",
    "        #if only one review is left to analyse\n",
    "        \n",
    "        #check concept between product title and the review\n",
    "        \n",
    "        text = [text,str(df.loc[review_id[0]][\"product_title\"])] \n",
    "        #add product_title and review to the list \n",
    "        \n",
    "        sentence_scores = LSA(text) \n",
    "        #call LSA method to get the score\n",
    "        \n",
    "        if(sentence_scores[0]==0): \n",
    "        #if review score is 0, then it's fake\n",
    "            remove_reviews.append(review_id[0])\n",
    "        continue\n",
    "    \n",
    "    #list of scores of reviews of same product\n",
    "    sentence_scores = LSA(reviews)\n",
    "            \n",
    "    for j in range(len(sentence_scores)):\n",
    "        #iterating through all the scores\n",
    "        \n",
    "        if(sentence_scores[j]==0.00):\n",
    "            # if score is 0, it's fake\n",
    "            remove_reviews.append(review_id[j])\n",
    "\n",
    "    # Content similarity check\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "    for i in range(len(reviews)):\n",
    "        sim_list = cosine_similarity(tfidf_matrix[i:i+1], tfidf_matrix)[0]\n",
    "        for k in range(len(sim_list)):\n",
    "            if k != i and sim_list[k] > 0.6:\n",
    "                if review_id[k] not in remove_reviews:\n",
    "                    remove_reviews.append(review_id[k])\n",
    "\n",
    "# Output the results\n",
    "print(f\"Total fake reviews detected: {len(remove_reviews)}\")\n",
    "print(\"Fake review IDs:\", remove_reviews)\n",
    "\n",
    "# Save to pickle\n",
    "with open('fake_reviews.pkl', 'wb') as f:\n",
    "    pickle.dump(remove_reviews, f)\n",
    "print(\"Results saved to fake_reviews.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
